{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize and Analyze Startup Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "To prepare your environment, you need to install some packages and enter credentials for the Watson services."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Install the necessary packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Watson Developer Cloud Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install watson-developer-cloud==1.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install IBM Database Server Python Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ibm_db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Other Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install cssselect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install fake-useragent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Import packages and libraries\n",
    "\n",
    "Import the packages and libraries that you'll use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time\n",
    "from random import randint\n",
    "from IPython.display import display, HTML\n",
    "#import selenium\n",
    "#from selenium import webdriver\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "from watson_developer_cloud import NaturalLanguageUnderstandingV1\n",
    "from watson_developer_cloud.natural_language_understanding_v1 \\\n",
    "  import Features, EntitiesOptions, KeywordsOptions, SemanticRolesOptions, SentimentOptions, EmotionOptions, ConceptsOptions, CategoriesOptions\n",
    "\n",
    "import ibm_boto3\n",
    "from botocore.client import Config\n",
    "\n",
    "\n",
    "import json\n",
    "import nltk\n",
    "import csv\n",
    "import ibm_db\n",
    "import sys\n",
    "if sys.version_info[0] < 3: \n",
    "    from StringIO import StringIO\n",
    "else:\n",
    "    from io import StringIO\n",
    "from io import BytesIO\n",
    "\n",
    "from urllib.parse import urlencode, urlparse, parse_qs\n",
    "\n",
    "from lxml.html import fromstring\n",
    "from requests import get\n",
    "from fake_useragent import UserAgent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "Add configurable items of the notebook below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Add your service credentials from IBM Natural Language Understanding service\n",
    "You must create a Watson Natural Language Understanding service on IBM Cloud. Create a service for Natural Language Understanding (NLU). Insert the apikey and url values in the variables, for your NLU in the following cell. Do not change the values of the version fields.\n",
    "\n",
    "Run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apikey=''\n",
    "url=''\n",
    "natural_language_understanding = NaturalLanguageUnderstandingV1(\n",
    "    version='2018-03-16',\n",
    "    iam_api_key=apikey,\n",
    "    url=url\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Add your service credentials for DB2\n",
    "\n",
    "Insert the DB2 service credentials as credentials_1 in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# @hidden_cell\n",
    "# The following code contains the credentials for a connection in your Project.\n",
    "# You might want to remove those credentials before you share your notebook.\n",
    "credentials_1 = {\n",
    "}\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Scrape Startup Information\n",
    "\n",
    "Scrapes data based on its appears on google for the following -\n",
    "\n",
    "* How many times it has appeared on News?\n",
    "* Whether it has a Wikipedia page or not?\n",
    "* Whether they have Tech blogs or not?\n",
    "* Whether they are active on Social Media (Twitter, Medium, etc..)?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insert Pandas Dataframe of the `companies_list.json file`\n",
    "\n",
    "Ensure the dataframe is saved as `df_data_1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "company_list_final=list(df_data_1['companies'])\n",
    "company_list_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def scrape_news_summaries_google(s):\n",
    "    ua = UserAgent()\n",
    "    \n",
    "    number_result=10\n",
    "    google_url = \"https://www.google.com/search?q=\" + s + \"&num=\" + str(number_result)\n",
    "    response = requests.get(google_url, {\"User-Agent\": ua.random})\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    result_div = soup.find_all('div', attrs = {'class': 'ZINbbc'})\n",
    "\n",
    "    \n",
    "    news_items=[]\n",
    "    for r in result_div:\n",
    "        # Checks if each element is present, else, raise exception\n",
    "        \n",
    "        try:\n",
    "            news_dict=dict()\n",
    "            link = r.find('a', href = True)\n",
    "            title = r.find('div', attrs={'class':'vvjwJb'}).get_text()\n",
    "            description = r.find('div', attrs={'class':'s3v9rd'}).get_text()\n",
    "\n",
    "            # Check to make sure everything is present before appending\n",
    "            if link != '' and title != '' and description != '': \n",
    "                \n",
    "                news_dict['news_link']=link['href']\n",
    "                news_dict['summary']=description\n",
    "                news_items.append(news_dict)\n",
    "        # Next loop if one element is not present\n",
    "        except:\n",
    "            continue\n",
    "    return news_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_rows=[]\n",
    "val=dict()\n",
    "for c in company_list_final:\n",
    "    for key,value in c.items():\n",
    "        s='\"'+key+'\"'+'company economic times'\n",
    "        inner_dict=dict()\n",
    "        temp=[]\n",
    "        temp=temp+scrape_news_summaries_google(s)\n",
    "        inner_dict['Description']=value[0]\n",
    "        inner_dict['Company_Link']=value[1]\n",
    "        inner_dict['News_Items']=temp\n",
    "        val[key]=inner_dict\n",
    "        final_rows.append(val)\n",
    "        val=dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_rows[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sentences(text):\n",
    "    \"\"\" Split text into sentences.\n",
    "    \"\"\"\n",
    "    sentence_delimiters = re.compile(u'[\\\\[\\\\]\\n.!?]')\n",
    "    sentences = sentence_delimiters.split(text)\n",
    "    return sentences\n",
    "\n",
    "def split_into_tokens(text):\n",
    "    \"\"\" Split text into tokens.\n",
    "    \"\"\"\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "def load_string(fileobject):\n",
    "    '''Load the file contents into a Python string'''\n",
    "    text = fileobject.read()\n",
    "    return text.decode('utf-8')\n",
    "\n",
    "def POS_tagging(text):\n",
    "    \"\"\" Generate Part of speech tagging of the text.\n",
    "    \"\"\"\n",
    "    POSofText = nltk.tag.pos_tag(text)\n",
    "    return POSofText\n",
    "\n",
    "def resolve_coreference(text, config):\n",
    "    \"\"\" Resolve coreferences in the text for Nouns that are Subjects in a sentence\n",
    "    \"\"\"\n",
    "    sentenceList = split_sentences(text)\n",
    "    referenceSubject = ''\n",
    "    sentenceText = ''\n",
    "    configjson = json.loads(config)\n",
    "    \n",
    "    for sentences in sentenceList:    \n",
    "        tokens = split_into_tokens(sentences)   \n",
    "        postags = POS_tagging(tokens)\n",
    "        sentencetags = chunk_sentence(postags)\n",
    "        subjects = find_subject(sentencetags)\n",
    "        for rules in configjson['configuration']['coreference']['rules']:\n",
    "            if (rules['type'] == 'chunking'):\n",
    "                for tags in rules['chunk']:\n",
    "                    chunktags = chunk_tagging(tags['tag'],tags['pattern'],postags)\n",
    "                    if (len(chunktags)>0):\n",
    "                        for words in chunktags:\n",
    "                            if tags['tag'] == 'PRP':\n",
    "                                if subjects == '':\n",
    "                                    sentenceText = sentenceText+sentences.replace(words,referenceSubject)+'. '\n",
    "                            elif tags['tag'] == 'NAME':\n",
    "                                if words == subjects:\n",
    "                                    referenceSubject = words\n",
    "                                    sentenceText = sentenceText+sentences+'. '\n",
    "                    \n",
    "    return sentenceText\n",
    "\n",
    "def chunk_sentence(text):\n",
    "    \"\"\" Tag the sentence using chunking.\n",
    "    \"\"\"\n",
    "    grammar = \"\"\"\n",
    "      NP: {<DT|JJ|PRP|NN.*>+} # Chunk sequences of DT,JJ,NN\n",
    "          #}<VB*|DT|JJ|RB|PRP><NN.*>+{  # Chink sequences of VB,DT,JJ,NN       \n",
    "      PP: {<IN><NP>}               # Chunk prepositions followed by NP\n",
    "      V: {<V.*>}                   # Verb      \n",
    "      VP: {<VB*><NP|PP|CLAUSE>+}  # Chunk verbs and their arguments\n",
    "      CLAUSE: {<NP><VP>}           # Chunk NP, VP\n",
    "      \"\"\"  \n",
    "    parsed_cp = nltk.RegexpParser(grammar,loop=2)\n",
    "    pos_cp = parsed_cp.parse(text)\n",
    "    return pos_cp\n",
    "\n",
    "def find_subject(t):\n",
    "    for s in t.subtrees(lambda t: t.label() == 'NP'):\n",
    "        return find_attrs(s,'NP')\n",
    "    \n",
    "def find_attrs(subtree,phrase):\n",
    "    attrs = ''\n",
    "    if phrase == 'NP':\n",
    "        for nodes in subtree:\n",
    "            if nodes[1] in ['DT','PRP$','POS','JJ','CD','ADJP','QP','NP','NNP']:\n",
    "                attrs = attrs+' '+nodes[0]\n",
    "    return attrs   \n",
    "\n",
    "def chunk_tagging(tag,chunk,text):\n",
    "    \"\"\" Tag the text using chunking.\n",
    "    \"\"\"\n",
    "    parsed_cp = nltk.RegexpParser(chunk)\n",
    "    pos_cp = parsed_cp.parse(text)\n",
    "    chunk_list=[]\n",
    "    for root in pos_cp:\n",
    "        if isinstance(root, nltk.tree.Tree):               \n",
    "            if root.label() == tag:\n",
    "                chunk_word = ''\n",
    "                for child_root in root:\n",
    "                    chunk_word = chunk_word +' '+ child_root[0]\n",
    "                chunk_list.append(chunk_word)\n",
    "    return chunk_list\n",
    "\n",
    "def analyze_using_NLU(analysistext):\n",
    "    \"\"\" Extract results from Watson Natural Language Understanding for each news item\n",
    "    \"\"\"\n",
    "    res=dict()\n",
    "    response = natural_language_understanding.analyze( \n",
    "        text=analysistext,\n",
    "        features=Features(\n",
    "                          sentiment=SentimentOptions(),\n",
    "                          entities=EntitiesOptions(), \n",
    "                          keywords=KeywordsOptions(),\n",
    "                          emotion=EmotionOptions(),\n",
    "                          concepts=ConceptsOptions(),\n",
    "                          categories=CategoriesOptions(),\n",
    "                          ))\n",
    "    res['results']=response\n",
    "    return res\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hasET(company_name):\n",
    "    cnbcVal=0\n",
    "    cnbcLinks=[]\n",
    "    ET_link=[]\n",
    "    s='\"'+company_name+'\"'+' economic times'\n",
    "    res= scrape_news_summaries_google(s)\n",
    "    return res\n",
    "\n",
    "def hasTwitter(company_name):\n",
    "    cnbcVal=0\n",
    "    cnbcLinks=[]\n",
    "    ET_link=[]\n",
    "    s='\"'+company_name+'\"'+' twitter'\n",
    "    res= scrape_news_summaries_google(s)\n",
    "    return res\n",
    "\n",
    "\n",
    "def getTechAreaNews(article_text):\n",
    "    concept=''\n",
    "    relevance=''\n",
    "    if len(article_text) > 15:\n",
    "        NLUres=analyze_using_NLU(article_text)\n",
    "        \n",
    "        if len(NLUres['results']['concepts']) != 0:\n",
    "            concept=NLUres['results']['concepts'][0]['text']\n",
    "            relevance=NLUres['results']['concepts'][0]['relevance']\n",
    "        if len(NLUres['results']['sentiment']) != 0: \n",
    "            sentiment=NLUres['results']['sentiment']['document']['label']\n",
    "    return concept,relevance,sentiment\n",
    "\n",
    "def getTechArea(article_text):\n",
    "    concept=''\n",
    "    relevance=''\n",
    "    sentiment=''\n",
    "    if len(article_text) > 15:\n",
    "        NLUres=analyze_using_NLU(article_text)\n",
    "        if len(NLUres['results']['concepts']) != 0:\n",
    "            concept=NLUres['results']['concepts'][0]['text']\n",
    "            relevance=NLUres['results']['concepts'][0]['relevance']\n",
    "    return concept,relevance\n",
    "\n",
    "def hasWiki(s):\n",
    "    wikiVal=0\n",
    "    wikiLinks=[]\n",
    "    s=s.replace(' ','+')\n",
    "    link='https://en.wikipedia.org/w/index.php?search='+s+'&title=Special%3ASearch&go=Go'\n",
    "    r = requests.get(link)\n",
    "    print(r.status_code)\n",
    "    content = r.text\n",
    "    return content\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Collect Wiki\n",
    "\n",
    "Collects info on how many Companies have an existing Wikipedia page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikiList=[]\n",
    "for f in final_rows:\n",
    "    for name, info in f.items():\n",
    "        wiki=dict()\n",
    "        wiki['Company_Name']=name\n",
    "        wiki['Wiki_Concept'],wiki['Wiki_Confidence']=getTechArea(hasWiki(name))\n",
    "        wikiList.append(wiki)\n",
    "wikiList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = wikiList[0].keys()\n",
    "with open('Wiki.csv', 'w') as output_file:\n",
    "    dict_writer = csv.DictWriter(output_file, keys)\n",
    "    dict_writer.writeheader()\n",
    "    dict_writer.writerows(wikiList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Collect ET\n",
    "\n",
    "Collects info through google links on how many hits are articles are from ET for a particular company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ET=[]\n",
    "for f in final_rows:\n",
    "    for name, info in f.items():\n",
    "        temp=dict()\n",
    "        news=hasET(name)\n",
    "        for n in news:\n",
    "            flag=0\n",
    "            if 'summary' in n:\n",
    "                summary=n['summary']\n",
    "                flag=1\n",
    "            link=n['news_link']\n",
    "            temp=dict()\n",
    "\n",
    "            if 'economictimes' in link and flag:\n",
    "                    temp['Company_Name']=name\n",
    "                    temp['News_Link']=link\n",
    "                    temp['News_Concept'],temp['News_Relevance'],temp['News_Sentiment']=getTechAreaNews(summary)\n",
    "                    ET.append(temp)\n",
    "ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = ET[0].keys()\n",
    "with open('ET_final.csv', 'w') as output_file:\n",
    "    dict_writer = csv.DictWriter(output_file, keys)\n",
    "    dict_writer.writeheader()\n",
    "    dict_writer.writerows(ET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Collect Tech Area\n",
    "\n",
    "Suggests the major tech area of a company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tech_area=[]\n",
    "for f in final_rows:\n",
    "    for name,info in f.items():\n",
    "        temp=dict()\n",
    "        temp[\"Company_Name\"]=name\n",
    "        print(info)\n",
    "        temp[\"Technology\"],temp[\"Technology_Relevance\"]=getTechArea(info['Description'])\n",
    "        tech_area.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tech_area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = tech_area[0].keys()\n",
    "with open('Tech_Area_Final.csv', 'w') as output_file:\n",
    "    dict_writer = csv.DictWriter(output_file, keys)\n",
    "    dict_writer.writeheader()\n",
    "    dict_writer.writerows(tech_area)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Collect Twitter\n",
    "\n",
    "Collects info on how many Tweets appear on Google Search of a Company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Twitter=[]\n",
    "for f in final_rows:\n",
    "    for name, info in f.items():\n",
    "        temp=dict()\n",
    "        news=hasTwitter(name)\n",
    "        for n in news:\n",
    "            flag=0\n",
    "            if 'summary' in n:\n",
    "                summary=n['summary']\n",
    "                flag=1\n",
    "            #print(summary)\n",
    "            link=n['news_link']\n",
    "            temp=dict()\n",
    "            #print('economictimes' in link)\n",
    "            if 'twitter.com/'+name.lower() in link and flag:\n",
    "                    temp['Company_Name']=name\n",
    "                    temp['Twitter_news_link']=link\n",
    "                    temp['Twitter_Topic'],temp['Twitter_Relevance'],temp['Twitter_Sentiment']=getTechAreaNews(summary)\n",
    "                    Twitter.append(temp)\n",
    "Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = Twitter[0].keys()\n",
    "with open('Twitter.csv', 'w') as output_file:\n",
    "    dict_writer = csv.DictWriter(output_file, keys)\n",
    "    dict_writer.writeheader()\n",
    "    dict_writer.writerows(Twitter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Combine the Results and Save to SPSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"Tech_Area_Final.csv\", \"r\")\n",
    "Tech_area = pd.read_csv(file, delimiter=',')\n",
    "file = open(\"ET_final.csv\", \"r\")\n",
    "ET = pd.read_csv(file, delimiter=',')\n",
    "file = open(\"Wiki.csv\", \"r\")\n",
    "Wiki = pd.read_csv(file, delimiter=',')\n",
    "file = open(\"Twitter.csv\", \"r\")\n",
    "Twitter = pd.read_csv(file, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = ET.append(Wiki)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b=a.append(Twitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compiled_rows=pd.merge(b,Tech_area, on=\"Company_Name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compiled_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compiled_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compiled_rows['Wiki_Confidence'].fillna(0.0, inplace=True)\n",
    "compiled_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compiled_rows['News_Relevance'].fillna(0.0, inplace=True)\n",
    "compiled_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compiled_rows['Technology_Relevance'].fillna(0.0, inplace=True)\n",
    "compiled_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compiled_rows['Twitter_Relevance'].fillna(0.0, inplace=True)\n",
    "compiled_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "compiled_rows = compiled_rows.replace(np.nan, '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make sure compiled_rows.Company_Name.unique() <= 10. Since SPSS Modeller evaluates only 10 rows at a time\n",
    "sample_len=int(10/len(list(compiled_rows.Company_Name.unique())))\n",
    "sample_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compiled_rows.groupby('Company_Name').apply(lambda x: x.sample(sample_len)).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store and Add table in DB2 Warehouse "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsn_driver = \"IBM DB2 ODBC DRIVER\"\n",
    "dsn_database = credentials_1['db'] \n",
    "dsn_hostname = credentials_1['host']\n",
    "dsn_port = 50000               \n",
    "dsn_uid = credentials_1['username']      \n",
    "dsn_pwd = credentials_1['password']\n",
    "\n",
    "dsn = (\n",
    "    \"DRIVER={{IBM DB2 ODBC DRIVER}};\"\n",
    "    \"DATABASE=\"+str(dsn_database)+\";\"\n",
    "    \"HOSTNAME=\"+str(dsn_hostname)+\";\"\n",
    "    \"PORT=\"+str(dsn_port)+\";\"\n",
    "    \"PROTOCOL=TCPIP;\"\n",
    "    \"UID=\"+str(dsn_uid)+\";\"\n",
    "    \"PWD=\"+str(dsn_pwd)+\";\").format(dsn_database, dsn_hostname, dsn_port, dsn_uid, dsn_pwd)\n",
    "\n",
    "conn = ibm_db.connect(dsn, \"\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_statement=pd.io.sql.get_schema(compiled_rows.reset_index(), 'DATA_FOR_SPSS')\n",
    "create_statement=create_statement.replace('TEXT', 'VARCHAR(500)')\n",
    "ibm_db.exec_immediate(conn, create_statement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuple_of_tuples = tuple([tuple(x) for x in compiled_rows.values])\n",
    "i=1\n",
    "for x in compiled_rows.values:\n",
    "    vals= (i,) + tuple(x)\n",
    "    print(vals)\n",
    "    sql = \"INSERT INTO \"+dsn_uid+\".DATA_FOR_SPSS VALUES\"+ str(vals)\n",
    "    i=i+1\n",
    "    ins_sql=ibm_db.prepare(conn, sql)\n",
    "    ibm_db.execute(ins_sql)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_statement= 'CREATE TABLE \"DATA_FOR_COGNOS\" (\\n\"index\" SMALLINT,\\n \"Company_Name\" VARCHAR,\\n  \"News_Concept\" VARCHAR,\\n  \"News_Link\" VARCHAR, \\n  \"News_Relevance\" DECFLOAT,\\n \"Overall_Sentiment\" VARCHAR ,\\n  \"Twitter_Topic\" VARCHAR,\\n  \"Twitter_news_link\" VARCHAR,\\n  \"Wiki_Concept\" VARCHAR,\\n \"Wiki_Confidence\" VARCHAR,\\n  \"Technology\" VARCHAR,\\n  \"Technology_Relevance\" DECFLOAT,\\n \"Company_News_Sentiments\" SMALLINT\\n)'\n",
    "create_statement=create_statement.replace('VARCHAR', 'VARCHAR(500)')\n",
    "ibm_db.exec_immediate(conn, create_statement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
